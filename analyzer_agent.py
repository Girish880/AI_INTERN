# backend/agents/analyzer_agent.py
import os
import json
import logging
from typing import List, Dict, Any, Tuple

from utils.report_generator import generate_report

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

logger = logging.getLogger("AnalyzerAgent")


class AnalyzerAgent:
    """
    AnalyzerAgent validates test execution results, performs reproducibility checks,
    and generates a structured JSON report using OpenAI's LLM.
    """

    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.2):
        # --- Directly set your API key here ---
        self.api_key = "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"
  

        if not self.api_key:
            raise RuntimeError(
                "âŒ OPENAI_API_KEY not found. Please set self.api_key in AnalyzerAgent.__init__"
            )

        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            api_key=self.api_key,
        )

    async def analyze_and_write_report(
        self, run_id: str, results: List[Dict[str, Any]]
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Analyze test results and generate a JSON report.
        :param run_id: Unique identifier for this run
        :param results: Execution results from OrchestratorAgent
        :return: (report_path, report_dict)
        """
        logger.info("AnalyzerAgent analyzing %d results for run %s", len(results), run_id)

        analyzed_tests: List[Dict[str, Any]] = []
        summary = {"total": len(results), "passed": 0, "failed": 0, "flaky": 0}

        # --- Initial local analysis ---
        for r in results:
            test_id = r.get("test_id")
            artifacts = r.get("artifacts", {})

            if "error" in artifacts:
                verdict = "failed"
                notes = f"Execution error: {artifacts['error']}"
                summary["failed"] += 1
            else:
                verdict = "passed"
                notes = "Execution completed, artifacts captured."
                summary["passed"] += 1

            analyzed_tests.append(
                {
                    "test_id": test_id,
                    "name": r.get("name", f"test_{test_id}"),
                    "verdict": verdict,
                    "artifacts": artifacts,
                    "reproducibility": {"repeats": 1, "stable": (verdict == "passed")},
                    "notes": notes,
                }
            )

        # --- LLM-based analysis ---
        try:
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        "You are a QA report analyzer. Validate test results for an online puzzle game."
                    ),
                    (
                        "user",
                        f"Here are the raw execution results:\n{results}\n\n"
                        "Analyze them for correctness, reproducibility, and likely causes of failure. "
                        "Output structured JSON with 'summary' and 'tests'."
                    ),
                ]
            )

            chain = prompt | self.llm
            response = await chain.ainvoke({})
            text = response.content
            smart_report = json.loads(text)

            if isinstance(smart_report, dict) and "tests" in smart_report:
                analyzed_tests = smart_report["tests"]
                summary = smart_report.get("summary", summary)
                logger.info("AnalyzerAgent used LLM for report")

        except Exception as e:
            logger.error("AnalyzerAgent LLM analysis failed: %s", e)
            logger.warning("Falling back to local analysis results")

        # --- Save report using utils.report_generator ---
        artifacts_map = {t["test_id"]: t["artifacts"] for t in analyzed_tests}
        report_path = generate_report(
            run_id=run_id,
            test_results=analyzed_tests,
            artifacts=artifacts_map,
            notes="Generated by AnalyzerAgent",
        )

        with open(report_path, "r", encoding="utf-8") as fh:
            report = json.load(fh)

        logger.info("AnalyzerAgent wrote report to %s", report_path)
        return report_path, report
